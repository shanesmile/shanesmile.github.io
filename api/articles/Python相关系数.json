{"title":"Python相关系数","slug":"Python相关系数","date":"2019-09-28T05:23:59.000Z","updated":"2019-09-28T05:36:01.266Z","comments":true,"path":"api/articles/Python相关系数.json","photos":[],"link":"","excerpt":"相关性是使用最广泛的统计概念之一。本节课介绍几种相关性的原理和内涵，同时也展示scipy.stats和pandas计算相关性的方法。最后是课堂联系题。","covers":null,"content":"<p>相关性是使用最广泛的统计概念之一。本节课介绍几种相关性的原理和内涵，同时也展示scipy.stats和pandas计算相关性的方法。最后是课堂联系题。</p>\n<a id=\"more\"></a>\n<h1 id=\"1-相关性（Correlation）\"><a href=\"#1-相关性（Correlation）\" class=\"headerlink\" title=\"1. 相关性（Correlation）\"></a>1. 相关性（Correlation）</h1><p>相关性指的是变量之间的相互关系或关联性。在数据挖掘中，揭示变量间的相关性是非常有意义的。例如，广告投放量的增加可能会引起特定商品销量的增加，某个电商平台的平均消费水平与消费者的特定因素的关联关系。一般情况下，相关性分析是理解变量之间的关系并建立模型的基础工作。</p>\n<p>简单的说，</p>\n<p>相关性可以帮助根据给定变量预测目标变量，</p>\n<p>相关性可能意味着因果关系的存在，</p>\n<p>相关性是基本统计量，是很多建模方法的基础。</p>\n<p>下文主要展开介绍：皮尔逊相关系数(Pearson Correlation Coefficient)、斯皮尔曼相关系数(Spearman’s correlation Coefficient)、肯德尔相关系数(Kendall’s Tau Coefficient)。</p>\n<h1 id=\"2-协方差-covariance\"><a href=\"#2-协方差-covariance\" class=\"headerlink\" title=\"2. 协方差(covariance)\"></a>2. 协方差(covariance)</h1><p>首先，先了解下协方差/共变数(covariance)。协方差是衡量两个随机变量𝑋和𝑌。</p>\n<p>𝐶𝑜𝑣(𝑋,𝑌)=𝐸[(𝑋−𝐸[𝑋])(𝑌−𝐸[𝑌])]=∑𝑛𝑖=1(𝑋𝑖−𝑋⎯⎯⎯⎯⎯)(𝑌𝑖−𝑌⎯⎯⎯⎯)𝑛−1<br>从公式中可以看出，每个变量与其均值相减，这些一一对应的中心化数值的乘积可以用来衡量一个随机变量的增加是不是与另一个随机变量的增加相关，最后上述乘积的期望即为两个变量相关性的总和。直观的，中心化数值的乘积可以认为是每个样本点到均值的距离构成的矩形的面积（两个距离分别表示矩形的两条边）。</p>\n<p>如果两个变量朝同一个方向变化，则连接点(𝑋𝑖,𝑌𝑖)和均值的(𝑋⎯⎯⎯⎯⎯,𝑌⎯⎯⎯⎯)矩形应该有更大的正值向量，也即更大的正值乘积。如果两个变量往不同的方向变化，则矩形的对角向量则是更大的负值，也即更大的负值乘积。如果两个变量不相关，向量平均上将会抵消，总体的对角向量数值量级接近0.</p>\n<p>协方差的问题是保留了变量𝑋和𝑌的数值量纲，因此取值范围随着两个变量的数值量纲变化，使得协方差无法对比不同组的变量的相关性的差别。例如，对于𝐶𝑜𝑣(𝑋,𝑌)=5.2, 𝐶𝑜𝑣(𝑍,𝑄)=3.6，可以看出𝑋和𝑌，𝑍和𝑄是正相关的，但是如果不参考两组变量的均值和分布，仅根据协方差，就无法判断𝑋和𝑌的相关性与𝑍和𝑄的相关性孰强孰弱。这种情况下，根据变量的一些统计特征计算得到标准化的相关系数，能够获取直观且量纲一致的相关性解释，就显得非常有用。</p>\n<h1 id=\"3-皮尔逊相关系数-Pearson-Correlation-Coefficient\"><a href=\"#3-皮尔逊相关系数-Pearson-Correlation-Coefficient\" class=\"headerlink\" title=\"3. 皮尔逊相关系数(Pearson Correlation Coefficient)\"></a>3. 皮尔逊相关系数(Pearson Correlation Coefficient)</h1><p>皮尔逊相关系数是使用最广泛的相关性度量方法，可以衡量连续变量的线性相关性。换言之，皮尔逊相关系数描述的是两个变量间的关系能够用直线表示的程度。值得一提的是，尽管有很多相关性的衡量方法，但由Karl Pearson在120年前提出的皮尔逊相关系数依然有着广泛的使用。</p>\n<p>接下来，介绍皮尔逊相关性(𝜌)的计算方法和直观解释。最初的皮尔逊相关系数计算公式根据变量和变量的均值计算得到。</p>\n<p>𝜌𝑋,𝑌=(𝑋𝑖−𝑋⎯⎯⎯⎯⎯)(𝑌𝑖−𝑌⎯⎯⎯⎯)∑(𝑋𝑖−𝑋⎯⎯⎯⎯⎯)2∑(𝑌𝑖−𝑌⎯⎯⎯⎯)2⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯√<br>观察值减去均值，然后使用标准差做数值的标准化变换。</p>\n<p>另外一种计算方法是，使用期望𝜇𝑋,𝜇𝑌和标准差𝛿𝑋,𝛿𝑌计算:</p>\n<p>𝜌𝑋,𝑌=𝐸(𝑋𝑖−𝜇𝑋)(𝑌𝑖−𝜇𝑌)𝛿𝑋𝛿𝑌<br>上述两个公式本质上是一致的。分母中的标准差的乘积保证了相关系数的值域为[−1,1]，使得相关性的解释变得直观和容易。</p>\n<p>下面三个图展示了三个皮尔逊相关系数的示例。𝜌越接近1，两个变量的正相关关系越强。反之，𝜌越接近-1，两个变量的负相关关系越强。如果两个变量相互独立，则𝜌越接近0。另外，即使皮尔逊相关系数很低，变量间也可能存在很强的相关性。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> scipy.stats <span class=\"keyword\">import</span> pearsonr</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\">​</span><br><span class=\"line\">x = [<span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]</span><br><span class=\"line\">y_1 = [<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>]</span><br><span class=\"line\">y_2 = list(<span class=\"number\">-1</span>*np.array(y_1))</span><br><span class=\"line\">y_3 = list(np.power(np.array(x)<span class=\"number\">-2.5</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\">corr_1, p_value_1 = pearsonr(x, y_1)</span><br><span class=\"line\">corr_2, p_value_2 = pearsonr(x, y_2)</span><br><span class=\"line\">corr_3, p_value_3 = pearsonr(x, y_3)</span><br><span class=\"line\">​</span><br><span class=\"line\">​</span><br><span class=\"line\">regressor = LinearRegression()  </span><br><span class=\"line\">regressor.fit(np.array(x).reshape(<span class=\"number\">-1</span>, <span class=\"number\">1</span>), np.array(y_1).reshape(<span class=\"number\">-1</span>, <span class=\"number\">1</span>)) <span class=\"comment\">#training the algorithm</span></span><br><span class=\"line\"><span class=\"comment\">#To retrieve the intercept:</span></span><br><span class=\"line\">intercept_1 = regressor.intercept_</span><br><span class=\"line\">coef_1 = regressor.coef_</span><br><span class=\"line\">​</span><br><span class=\"line\">regressor.fit(np.array(x).reshape(<span class=\"number\">-1</span>, <span class=\"number\">1</span>), np.array(y_2).reshape(<span class=\"number\">-1</span>, <span class=\"number\">1</span>)) <span class=\"comment\">#training the algorithm</span></span><br><span class=\"line\"><span class=\"comment\">#To retrieve the intercept:</span></span><br><span class=\"line\">intercept_2 = regressor.intercept_</span><br><span class=\"line\">coef_2 = regressor.coef_</span><br><span class=\"line\">​</span><br><span class=\"line\">regressor.fit(np.array(x).reshape(<span class=\"number\">-1</span>, <span class=\"number\">1</span>), np.array(y_3).reshape(<span class=\"number\">-1</span>, <span class=\"number\">1</span>)) <span class=\"comment\">#training the algorithm</span></span><br><span class=\"line\"><span class=\"comment\">#To retrieve the intercept:</span></span><br><span class=\"line\">intercept_3 = regressor.intercept_</span><br><span class=\"line\">coef_3 = regressor.coef_</span><br><span class=\"line\">​</span><br><span class=\"line\">fitted_1 = coef_1*np.array(x)+intercept_1</span><br><span class=\"line\">fitted_2 = coef_2*np.array(x)+intercept_2</span><br><span class=\"line\">fitted_3 = coef_3*np.array(x)+intercept_3</span><br><span class=\"line\">​</span><br><span class=\"line\">print(y_1,corr_1,p_value_1)</span><br><span class=\"line\">print(y_2,corr_2,p_value_2)</span><br><span class=\"line\">print(y_3,corr_3,p_value_3)</span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>, <span class=\"number\">9</span>, <span class=\"number\">10</span>] <span class=\"number\">0.9871352947927471</span> <span class=\"number\">0.0002471864016312273</span></span><br><span class=\"line\">[<span class=\"number\">0</span>, <span class=\"number\">-2</span>, <span class=\"number\">-4</span>, <span class=\"number\">-5</span>, <span class=\"number\">-9</span>, <span class=\"number\">-10</span>] <span class=\"number\">-0.9871352947927471</span> <span class=\"number\">0.0002471864016312273</span></span><br><span class=\"line\">[<span class=\"number\">6.25</span>, <span class=\"number\">2.25</span>, <span class=\"number\">0.25</span>, <span class=\"number\">0.25</span>, <span class=\"number\">2.25</span>, <span class=\"number\">6.25</span>] <span class=\"number\">0.0</span> <span class=\"number\">1.0</span></span><br><span class=\"line\">fig,axes = plt.subplots(<span class=\"number\">1</span>,<span class=\"number\">3</span>,figsize=[<span class=\"number\">16</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">axes[<span class=\"number\">0</span>].scatter(x,y_1)</span><br><span class=\"line\">axes[<span class=\"number\">0</span>].plot(x,fitted_1.flat,<span class=\"string\">'r-'</span>)</span><br><span class=\"line\">axes[<span class=\"number\">0</span>].text(<span class=\"number\">1</span>,<span class=\"number\">8</span>,<span class=\"string\">'&#123;:.2f&#125;, &#123;:.2f&#125;, &#123;:.2f&#125;, &#123;:.2f&#125;'</span>.format(coef_1[<span class=\"number\">0</span>][<span class=\"number\">0</span>],corr_1,np.std(x),np.std(y_1)))</span><br><span class=\"line\">axes[<span class=\"number\">1</span>].scatter(x,y_2)</span><br><span class=\"line\">axes[<span class=\"number\">1</span>].plot(x,fitted_2.flat,<span class=\"string\">'r-'</span>)</span><br><span class=\"line\">axes[<span class=\"number\">1</span>].text(<span class=\"number\">2</span>,<span class=\"number\">-2</span>,<span class=\"string\">'&#123;:.2f&#125;, &#123;:.2f&#125;, &#123;:.2f&#125;, &#123;:.2f&#125;'</span>.format(coef_2[<span class=\"number\">0</span>][<span class=\"number\">0</span>],corr_2,np.std(x),np.std(y_2)))</span><br><span class=\"line\">axes[<span class=\"number\">2</span>].scatter(x,y_3)</span><br><span class=\"line\">axes[<span class=\"number\">2</span>].plot(x,fitted_3.flat,<span class=\"string\">'r-'</span>)</span><br><span class=\"line\">axes[<span class=\"number\">2</span>].text(<span class=\"number\">1</span>,<span class=\"number\">5</span>,<span class=\"string\">'&#123;:.2f&#125;, &#123;:.2f&#125;, &#123;:.2f&#125;, &#123;:.2f&#125;'</span>.format(coef_3[<span class=\"number\">0</span>][<span class=\"number\">0</span>],corr_3,np.std(x),np.std(y_3)))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p>在上面的三个示例中，红色的直线是通过线性模型拟合得到的最佳拟合结果（使得所有点到拟合直线的距离的和最小），实际上皮尔逊相关性与拟合直线的斜率有明确的联系，<br>𝜌=𝑏𝑠𝑥𝑠𝑦<br>这里𝑏指的是拟合直线的斜率，𝑠𝑥和𝑠𝑦指的是𝑋和𝑌的标准差。换言之，相关性反映的是变量间变化性的多少和联系。皮尔逊相关性与拟合直线斜率的关系清楚地表明：</p>\n<p>皮尔逊相关性只能表示线性关系。<br>相关性在预测模型构建中有着重要的作用。<br>需要指出的是，我们并没有对于𝑋和𝑌的分布给出任何假设。唯一的限定是皮尔逊相关性假设两个变量存在线性关系。皮尔逊相关性依赖于变量的均值和标准差，这可能导致其对可能存在的异常值非常敏感。</p>\n<p>另外一种解释皮尔逊相关性的方法是决定系数𝑅2。因为皮尔逊相关系数是无单位的，它的平方表示𝑌的变化可以由𝑋解释的比例。当𝜌=−0.65时，𝑅2=(−0.65)2∗100%≈42%，这表示𝑌的42%的变化可以由𝑋解释。</p>\n<h1 id=\"4-斯皮尔曼相关系数-Spearman’s-Correlation-Coefficient\"><a href=\"#4-斯皮尔曼相关系数-Spearman’s-Correlation-Coefficient\" class=\"headerlink\" title=\"4.斯皮尔曼相关系数(Spearman’s Correlation Coefficient)\"></a>4.斯皮尔曼相关系数(Spearman’s Correlation Coefficient)</h1><p>斯皮尔曼相关系数(Spearman’s Correlation Coefficient)和斯皮尔曼等级相关系数(Spearman’s rank correlation coefficient)指代的同样的概念，可以认为皮尔逊相关系数的一种特殊情况，用于计算有序变量的相关性。斯皮尔曼相关性不局限于线性关系，相反，它衡量的是变量间的单调变化的关联关系（如单调递增或单调递减），依赖于数值的大小顺序。皮尔逊相关性依赖于变量的均值和标准差，而斯皮尔曼相关性只关注变量数值的相对顺序。这个特性使得斯皮尔曼相关性适用于连续的和非连续的数据。</p>\n<p>斯皮尔曼相关系数的形式上与皮尔逊相关系数类似，不同的是基于原始数值的排序进行计算，</p>\n<p>𝜌𝑟𝑎𝑛𝑘𝑋,𝑟𝑎𝑛𝑘𝑌=𝑐𝑜𝑣(𝑟𝑎𝑛𝑘𝑋,𝑟𝑎𝑛𝑘𝑌)𝜎𝑟𝑎𝑛𝑘𝑋𝜎𝑟𝑎𝑛𝑘𝑌<br>如果所有的数值排序没有并列的情况，还可以通过以下方式计算：</p>\n<p>𝜌𝑠=1−6∑𝑑2𝑖𝑁(𝑁2−1)<br>这里𝑑𝑖=𝑟𝑎𝑛𝑘(𝑋𝑖)−𝑟𝑎𝑛𝑘(𝑌𝑖)表示变量对应观测值的排序的差值，𝑁是观测值的数量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># calculate the spearman's correlation between two variables</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> numpy.random <span class=\"keyword\">import</span> rand</span><br><span class=\"line\"><span class=\"keyword\">from</span> numpy.random <span class=\"keyword\">import</span> seed</span><br><span class=\"line\"><span class=\"keyword\">from</span> scipy.stats <span class=\"keyword\">import</span> spearmanr,pearsonr</span><br><span class=\"line\"><span class=\"comment\"># seed random number generator</span></span><br><span class=\"line\">seed(<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># prepare data</span></span><br><span class=\"line\">x = np.linspace(<span class=\"number\">0</span>,<span class=\"number\">4</span>,<span class=\"number\">25</span>)</span><br><span class=\"line\">y_1 = np.tanh(x)</span><br><span class=\"line\">y_2 = np.tan(x)</span><br><span class=\"line\">y_3 = <span class=\"number\">2</span>*np.power((x<span class=\"number\">-2</span>),<span class=\"number\">2</span>)</span><br><span class=\"line\">​</span><br><span class=\"line\">scoef_1, sp_1 = spearmanr(x, y_1)</span><br><span class=\"line\">scoef_2, sp_2 = spearmanr(x, y_2)</span><br><span class=\"line\">scoef_3, sp_3 = spearmanr(x, y_3)</span><br><span class=\"line\">​</span><br><span class=\"line\">coef_1, p_1 = pearsonr(x, y_1)</span><br><span class=\"line\">coef_2, p_2 = pearsonr(x, y_2)</span><br><span class=\"line\">coef_3, p_3 = pearsonr(x, y_3)</span><br><span class=\"line\">​</span><br><span class=\"line\">fig,axes = plt.subplots(<span class=\"number\">1</span>,<span class=\"number\">3</span>,figsize=[<span class=\"number\">16</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\">axes[<span class=\"number\">0</span>].scatter(x,y_1)</span><br><span class=\"line\">axes[<span class=\"number\">0</span>].text(<span class=\"number\">2</span>,<span class=\"number\">0.75</span>,<span class=\"string\">'&#123;:.2f&#125;, &#123;:.2f&#125;'</span>.format(scoef_1,coef_1))</span><br><span class=\"line\">axes[<span class=\"number\">1</span>].scatter(x,y_2)</span><br><span class=\"line\">axes[<span class=\"number\">1</span>].text(<span class=\"number\">2</span>,<span class=\"number\">10</span>,<span class=\"string\">'&#123;:.2f&#125;, &#123;:.2f&#125;'</span>.format(scoef_2,coef_2))</span><br><span class=\"line\">axes[<span class=\"number\">2</span>].scatter(x,y_3)</span><br><span class=\"line\">axes[<span class=\"number\">2</span>].text(<span class=\"number\">1</span>,<span class=\"number\">6</span>,<span class=\"string\">'&#123;:.2f&#125;, &#123;:.2f&#125;'</span>.format(scoef_3,coef_3))</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<p>使用上述三个例子对比皮尔逊相关性于斯皮尔曼相关性的差别。</p>\n<p>在第一幅图中，变量间有明显的非线性的单调递增关系。这种情况下，𝑋和𝑌的变量排序的位次将是一一对应的，因此斯皮尔逊相关系数是1。同时由于存在局部的近似线性关系，皮尔逊相关系数为0.82，也表明存在较强的线性关系。</p>\n<p>在第二幅图中，由于𝑋和𝑌的非单调性、非线性相关性，皮尔逊相关系数为-0.08，斯皮尔逊相关系数为-0.11。</p>\n<p>在第三幅图中，𝑋和𝑌是典型的以2为中心的二次方程关系，斯皮尔逊相关系数为-0.04，皮尔逊相关系数为0。</p>\n<p>上述三个示例表明，相关性分析对于分析数据的内在模式并不是万能的，即使是弱的皮尔逊相关性和斯皮尔逊相关性也不能否定其它关联性的存在。</p>\n<p>总体上，通过相关性分析和可视化的技术可以帮助探究变量间关系的隐含模式。</p>\n<h1 id=\"5-肯德尔相关系数-Kendall’s-Tau-Coefficient\"><a href=\"#5-肯德尔相关系数-Kendall’s-Tau-Coefficient\" class=\"headerlink\" title=\"5. 肯德尔相关系数(Kendall’s Tau Coefficient)\"></a>5. 肯德尔相关系数(Kendall’s Tau Coefficient)</h1><p>肯德尔相关系数也是基于变量的排序。但是与斯皮尔逊相关系数不同，肯德尔相关性不考虑排序的差异，只关注趋势的一致性。因此，肯德尔相关系数更适用于离散数值。</p>\n<p>肯德尔相关系数可以通过如下公式计算，</p>\n<p>𝜏=(number of concordant pairs)−(number of discordant pairs)𝑁(𝑁−1)/2<br>观察一组数据，有5组观察值（实际上，这种小数量的数据样本不足以得到有效可靠的结论，这里只是为了计算的简便性）。</p>\n<p>X    Y<br>a    1    7<br>b    2    5<br>c    3    1<br>d    4    6<br>e    5    9<br>使用(𝑥1,𝑦1)和(𝑥2,𝑦2)表示一组数据对，一致性的情况有：𝑥1&gt;𝑥2,𝑦1&gt;𝑦2和𝑥1&lt;𝑥2,𝑦1&lt;𝑦2，不满足上述情况的数据对为不一致。在上述数据中， (4,6)，(5,9)是一组一致的数据对， (1, 7)，(2, 5)是一组不一致的数据对。</p>\n<p>为了计算肯德尔系数𝜏，统计所有一致的和不一致的数据对的数量， 一致的数据对有6组：</p>\n<p>(1,7)和(5,9)</p>\n<p>(2,5)和(4,6)</p>\n<p>(2,5)和(5,9)</p>\n<p>(3,1)和(4,6)</p>\n<p>(3,1)和(5,9)</p>\n<p>(4,6)和(5,9)</p>\n<p>不一致的有4组：</p>\n<p>(1,7)和(2,5)</p>\n<p>(1,7)和(3,1)</p>\n<p>(1,7)和(4,6)</p>\n<p>(2,5)和(3,1)</p>\n<p>𝜏的计算公式中的分母是所有样本的组合数𝑁(𝑁−1)/2。 对于上述数据示例，可以计算得到</p>\n<p>𝜏=(6−4)5(5−1)/2=0.2<br>肯德尔相关性适用于离散数据，尤其是排位比数值差别更重要的情况。</p>\n<h1 id=\"6-使用Pandas计算相关性\"><a href=\"#6-使用Pandas计算相关性\" class=\"headerlink\" title=\"6. 使用Pandas计算相关性\"></a>6. 使用Pandas计算相关性</h1><p>选择University of California, Irvine(UCI)的一个关于汽车燃油效率的数据作为分析对象，使用pandas计算与汽车燃油效率(vehicle fuel efficiency)相关的一些变量。</p>\n<p>从UCI的在线知识库中读取数据。这个数据中使用?表示丢失的数据项。这里我们使用pandas.read_csv()读取数据。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">path = <span class=\"string\">'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'</span></span><br><span class=\"line\">​</span><br><span class=\"line\">mpg_data = pd.read_csv(path, delim_whitespace=<span class=\"literal\">True</span>, header=<span class=\"literal\">None</span>,</span><br><span class=\"line\">            names = [<span class=\"string\">'mpg'</span>, <span class=\"string\">'cylinders'</span>, <span class=\"string\">'displacement'</span>,<span class=\"string\">'horsepower'</span>,</span><br><span class=\"line\">            <span class=\"string\">'weight'</span>, <span class=\"string\">'acceleration'</span>, <span class=\"string\">'model_year'</span>, <span class=\"string\">'origin'</span>, <span class=\"string\">'name'</span>],</span><br><span class=\"line\">            na_values=<span class=\"string\">'?'</span>)</span><br><span class=\"line\">mpg_data.isnull().any()</span><br></pre></td></tr></table></figure>\n\n<p>mpg             False<br>cylinders       False<br>displacement    False<br>horsepower       True<br>weight          False<br>acceleration    False<br>model_year      False<br>origin          False<br>name            False<br>dtype: bool</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mpg_data.describe()</span><br></pre></td></tr></table></figure>\n\n<p>mpg    cylinders    displacement    horsepower    weight    acceleration    model_year    origin<br>count    398.000000    398.000000    398.000000    392.000000    398.000000    398.000000    398.000000    398.000000<br>mean    23.514573    5.454774    193.425879    104.469388    2970.424623    15.568090    76.010050    1.572864<br>std    7.815984    1.701004    104.269838    38.491160    846.841774    2.757689    3.697627    0.802055<br>min    9.000000    3.000000    68.000000    46.000000    1613.000000    8.000000    70.000000    1.000000<br>25%    17.500000    4.000000    104.250000    75.000000    2223.750000    13.825000    73.000000    1.000000<br>50%    23.000000    4.000000    148.500000    93.500000    2803.500000    15.500000    76.000000    1.000000<br>75%    29.000000    8.000000    262.000000    126.000000    3608.000000    17.175000    79.000000    2.000000<br>max    46.600000    8.000000    455.000000    230.000000    5140.000000    24.800000    82.000000    3.000000</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mpg_data.info()</span><br></pre></td></tr></table></figure>\n\n<p>&lt;class ‘pandas.core.frame.DataFrame’&gt;<br>RangeIndex: 398 entries, 0 to 397<br>Data columns (total 9 columns):<br>mpg             398 non-null float64<br>cylinders       398 non-null int64<br>displacement    398 non-null float64<br>horsepower      392 non-null float64<br>weight          398 non-null float64<br>acceleration    398 non-null float64<br>model_year      398 non-null int64<br>origin          398 non-null int64<br>name            398 non-null object<br>dtypes: float64(5), int64(3), object(1)<br>memory usage: 28.1+ KB<br>观察数据，可以发现horsepower属性项中有6个空值，相比于总量398条数据，可以选择直接丢弃这6条包含空值的数据；也可以选择某种方法替换空值，如使用该属性项的均值替换、前向填充、后向填充等。为了演示填充值的用法，这里使用horsepower的均值做替换。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#mpg_data.dropna(inplace=True)#丢弃包含空值的行</span></span><br><span class=\"line\">values = &#123;<span class=\"string\">'horsepower'</span>:<span class=\"number\">104.47</span>&#125;</span><br><span class=\"line\">mpg_data.fillna(value=values,inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">mpg_data.describe()</span><br></pre></td></tr></table></figure>\n\n<p>mpg    cylinders    displacement    horsepower    weight    acceleration    model_year    origin<br>count    398.000000    398.000000    398.000000    398.000000    398.000000    398.000000    398.000000    398.000000<br>mean    23.514573    5.454774    193.425879    104.469397    2970.424623    15.568090    76.010050    1.572864<br>std    7.815984    1.701004    104.269838    38.199187    846.841774    2.757689    3.697627    0.802055<br>min    9.000000    3.000000    68.000000    46.000000    1613.000000    8.000000    70.000000    1.000000<br>25%    17.500000    4.000000    104.250000    76.000000    2223.750000    13.825000    73.000000    1.000000<br>50%    23.000000    4.000000    148.500000    95.000000    2803.500000    15.500000    76.000000    1.000000<br>75%    29.000000    8.000000    262.000000    125.000000    3608.000000    17.175000    79.000000    2.000000<br>max    46.600000    8.000000    455.000000    230.000000    5140.000000    24.800000    82.000000    3.000000<br>pandas提供了简单的计算数据列之间相关性的方法。在燃油效率(mpg)的实例中，我们检查是不是更重(weight)的机动车趋向于有等低的燃油效率:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mpg_data[<span class=\"string\">'mpg'</span>].corr(mpg_data[<span class=\"string\">'weight'</span>])</span><br></pre></td></tr></table></figure>\n\n<p>-0.8317409332443354<br>如上述结果，机动车重量(weight)和燃油效率(mpg)似乎有着很强的负相关关系。对于其它变量间的关系，可以以类似的方法计算得到。另外，还可以指定相关性方法: Spearman, Pearson, 或Kendall。默认情况下是计算Pearson相关系数。下面，我们去掉model_year和origin两个属性项，计算剩余属性变量间的相关性。</p>\n<h1 id=\"pairwise-correlation\"><a href=\"#pairwise-correlation\" class=\"headerlink\" title=\"pairwise correlation\"></a>pairwise correlation</h1><p>mpg_data.drop([‘model_year’, ‘origin’], axis=1).corr(method=’spearman’)<br>mpg    cylinders    displacement    horsepower    weight    acceleration<br>mpg    1.000000    -0.821864    -0.855692    -0.843180    -0.874947    0.438677<br>cylinders    -0.821864    1.000000    0.911876    0.808620    0.873314    -0.474189<br>displacement    -0.855692    0.911876    1.000000    0.866670    0.945986    -0.496512<br>horsepower    -0.843180    0.808620    0.866670    1.000000    0.868659    -0.647557<br>weight    -0.874947    0.873314    0.945986    0.868659    1.000000    -0.404550<br>acceleration    0.438677    -0.474189    -0.496512    -0.647557    -0.404550    1.000000<br>pandas也支持表格的强调显示，便于直观地看出相关性的高低。在建模过程中，特别是对于建立回归模型，理解数据中存在的相关性是非常重要的。高度相关的预测因素，如多重共线性，将导致相关系数不可靠。下面是计算皮尔逊相关性的示例，使用有梯度区分的颜色标注相关性结果表格。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mpg_data.drop([<span class=\"string\">'model_year'</span>, <span class=\"string\">'origin'</span>], axis=<span class=\"number\">1</span>).corr(method=<span class=\"string\">'pearson'</span>).style.format(<span class=\"string\">\"&#123;:.2&#125;\"</span>).background_gradient(cmap=plt.get_cmap(<span class=\"string\">'coolwarm'</span>), axis=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<p>mpg    cylinders    displacement    horsepower    weight    acceleration<br>mpg    1.0    -0.78    -0.8    -0.77    -0.83    0.42<br>cylinders    -0.78    1.0    0.95    0.84    0.9    -0.51<br>displacement    -0.8    0.95    1.0    0.89    0.93    -0.54<br>horsepower    -0.77    0.84    0.89    1.0    0.86    -0.68<br>weight    -0.83    0.9    0.93    0.86    1.0    -0.42<br>acceleration    0.42    -0.51    -0.54    -0.68    -0.42    1.0<br>最后，为了可视化地展示mpg与weight,horsepower,acceleration的关系，我们画出这些变量数值的散点图，计算它们的皮尔逊相关系数和斯皮尔曼相关系数。实际的数据分析中，如果数据量过于庞大如百万数量级，可以先使用随机采样的方法选择少量的数据进行观察。</p>\n<p>在这个案例中，对于horsepower和weight，因为非线性关系，斯皮尔曼相关系数高于皮尔逊相关系数，而对于acceleration，并不存在显著的线性关系和单调变化一致性，斯皮尔曼相关系数和皮尔逊相关系数比较接近。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># plot correlated values</span></span><br><span class=\"line\">plt.rcParams[<span class=\"string\">'figure.figsize'</span>] = [<span class=\"number\">16</span>, <span class=\"number\">4</span>]</span><br><span class=\"line\">​</span><br><span class=\"line\">fig, ax = plt.subplots(nrows=<span class=\"number\">1</span>, ncols=<span class=\"number\">3</span>)</span><br><span class=\"line\">​</span><br><span class=\"line\">ax=ax.flatten()</span><br><span class=\"line\">​</span><br><span class=\"line\">cols = [<span class=\"string\">'weight'</span>, <span class=\"string\">'horsepower'</span>, <span class=\"string\">'acceleration'</span>]</span><br><span class=\"line\">colors=[<span class=\"string\">'#415952'</span>, <span class=\"string\">'#f35134'</span>, <span class=\"string\">'#243AB5'</span>, <span class=\"string\">'#243AB5'</span>]</span><br><span class=\"line\">j=<span class=\"number\">0</span></span><br><span class=\"line\">​</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> ax:</span><br><span class=\"line\">    <span class=\"keyword\">if</span> j==<span class=\"number\">0</span>:</span><br><span class=\"line\">        i.set_ylabel(<span class=\"string\">'MPG'</span>)</span><br><span class=\"line\">    i.scatter(mpg_data[cols[j]], mpg_data[<span class=\"string\">'mpg'</span>],  alpha=<span class=\"number\">0.5</span>, color=colors[j])</span><br><span class=\"line\">    i.set_xlabel(cols[j])</span><br><span class=\"line\">    i.set_title(<span class=\"string\">'Pearson: %s'</span>%mpg_data.corr().loc[cols[j]][<span class=\"string\">'mpg'</span>].round(<span class=\"number\">2</span>)+<span class=\"string\">' Spearman: %s'</span>%mpg_data.corr(method=<span class=\"string\">'spearman'</span>).loc[cols[j]][<span class=\"string\">'mpg'</span>].round(<span class=\"number\">2</span>))</span><br><span class=\"line\">    j+=<span class=\"number\">1</span></span><br><span class=\"line\">​</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"7-相关性和因果性（Correlation-and-Causation）\"><a href=\"#7-相关性和因果性（Correlation-and-Causation）\" class=\"headerlink\" title=\"7.相关性和因果性（Correlation and Causation）\"></a>7.相关性和因果性（Correlation and Causation）</h1><p>在上述分析中，燃油效率与相关因素的关系可以根据机动车机械学原理从直觉上进行良好的解释。但是，在实际的数据分析中，很多关系并非如此直观。一个广为人知的事实是，相关性并不代表因果性，因此，强的相关性需要更加严谨的考虑和分析。</p>\n<p>有个反面案例。德国的一个研究人员在一篇滑稽的论文中使用相关性的概念支持“婴儿是由鹳（stork）带来的”这样一个民间说法。下面的图中展示的是婴儿的出生数与鹳的数量的关系。 image.png</p>\n<p>左图黑色方形标识的实线表示的是鹳的数量的增加趋势，白色三角形标识的实线表示的是医院出生的婴儿数量；在右图中，医院外出生的婴儿数量与鹳的数量的增加趋势一致。根据这些序列的相关性，作者认为医院外出生的婴儿数量的增加趋势与鹳的增加、医院内出生婴儿的数量的减少趋势同时出现，这证明这德国的越来越多的婴儿时由鹳带来的。</p>\n<p>这虽然是个反面的例子，但它证明了重要的一点，因为偶然因素，在很多变量间都可以发现伪相关关系的存在。</p>\n<p>另外，高相关性也可能因为一些未观测到的变量展现出因果关系。例如，一个城市中食品商店的数量和冰淇淋奶油厂的数量非常相关。然而，这里存在一个隐藏的变量，城市的人口数量。 image.png</p>\n<p>这个例子告诉我们，相关性只是数据的一个统计概要信息，远不能完整地解释数据的全部。</p>\n<p>此外，还有其它相关性指标，如phi coefficient，可用于表示两个二元变量(dichotomous variable)的关联性度量。</p>\n<h1 id=\"8-练习\"><a href=\"#8-练习\" class=\"headerlink\" title=\"8. 练习\"></a>8. 练习</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">x = np.linspace(<span class=\"number\">1</span>,<span class=\"number\">20</span>,<span class=\"number\">20</span>)</span><br><span class=\"line\">sin = np.sin(x)</span><br><span class=\"line\">tanh = np.tanh(x)</span><br><span class=\"line\">possion = np.exp(-x)</span><br><span class=\"line\">data = &#123;<span class=\"string\">'x'</span>:x,<span class=\"string\">'sin'</span>:sin,<span class=\"string\">'tanh'</span>:tanh,<span class=\"string\">'possion'</span>:possion&#125;</span><br><span class=\"line\">df = pd.DataFrame(data,index=(x.astype(np.int32)<span class=\"number\">-1</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> index,row <span class=\"keyword\">in</span> df.iterrows():</span><br><span class=\"line\">    print(index,row[<span class=\"string\">'x'</span>],row[<span class=\"string\">'sin'</span>],row[<span class=\"string\">'tanh'</span>],row[<span class=\"string\">'possion'</span>])</span><br></pre></td></tr></table></figure>\n\n<p>0 1.0 0.8414709848078965 0.7615941559557649 0.36787944117144233<br>1 2.0 0.9092974268256817 0.9640275800758169 0.1353352832366127<br>2 3.0 0.1411200080598672 0.9950547536867305 0.049787068367863944<br>3 4.0 -0.7568024953079282 0.999329299739067 0.01831563888873418<br>4 5.0 -0.9589242746631385 0.9999092042625951 0.006737946999085467<br>5 6.0 -0.27941549819892586 0.9999877116507956 0.0024787521766663585<br>6 7.0 0.6569865987187891 0.9999983369439447 0.0009118819655545162<br>7 8.0 0.9893582466233818 0.9999997749296758 0.00033546262790251185<br>8 9.0 0.4121184852417566 0.999999969540041 0.00012340980408667956<br>9 10.0 -0.5440211108893698 0.9999999958776927 4.5399929762484854e-05<br>10 11.0 -0.9999902065507035 0.9999999994421064 1.670170079024566e-05<br>11 12.0 -0.5365729180004349 0.9999999999244973 6.14421235332821e-06<br>12 13.0 0.4201670368266409 0.9999999999897818 2.2603294069810542e-06<br>13 14.0 0.9906073556948704 0.9999999999986171 8.315287191035679e-07<br>14 15.0 0.6502878401571168 0.9999999999998128 3.059023205018258e-07<br>15 16.0 -0.2879033166650653 0.9999999999999747 1.1253517471925912e-07<br>16 17.0 -0.9613974918795568 0.9999999999999966 4.139937718785167e-08<br>17 18.0 -0.7509872467716762 0.9999999999999996 1.522997974471263e-08<br>18 19.0 0.14987720966295234 0.9999999999999999 5.602796437537268e-09<br>19 20.0 0.9129452507276277 1.0 2.061153622438558e-09</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">fig,axes=plt.subplots(<span class=\"number\">1</span>,<span class=\"number\">3</span>,figsize=(<span class=\"number\">16</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\">axes[<span class=\"number\">0</span>].plot(x,sin)</span><br><span class=\"line\">axes[<span class=\"number\">1</span>].plot(x,tanh)</span><br><span class=\"line\">axes[<span class=\"number\">2</span>].plot(x,possion)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\">df.to_csv(<span class=\"string\">'data.csv'</span>)</span><br></pre></td></tr></table></figure>\n\n<p>编程练习</p>\n<p>使用pandas读取data.csv，自己实现相关系数地计算方法（选择皮尔逊，斯皮尔曼，肯德尔中的一个实现），计算x和sin，tanh，possion三个变量间的相关系数。</p>\n","categories":[{"name":"Python","slug":"Python","count":3,"path":"api/categories/Python.json"},{"name":"数据挖掘","slug":"Python/数据挖掘","count":1,"path":"api/categories/Python/数据挖掘.json"}],"tags":[{"name":"Python","slug":"Python","count":3,"path":"api/tags/Python.json"},{"name":"数据挖掘","slug":"数据挖掘","count":1,"path":"api/tags/数据挖掘.json"}]}