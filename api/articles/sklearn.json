{"title":"Sklearn","slug":"sklearn","date":"2020-01-12T12:10:22.000Z","updated":"2020-11-10T08:36:31.190Z","comments":true,"path":"api/articles/sklearn.json","photos":[],"link":"","excerpt":"1<br>2<br>3<br>4<br>5<br>6<br>7<br>from sklearn import datasets<br># 导入数据<br>iris = datasets.load_iris()<br>X = iris.data   # 获取特征向量<br>Y = iris.target # 获取label<br># X, y = load_iris(return_X_y=True)<br># print(X.shape, y.shape, type(X))<br>","covers":null,"content":"<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"comment\"># 导入数据</span></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data   <span class=\"comment\"># 获取特征向量</span></span><br><span class=\"line\">Y = iris.target <span class=\"comment\"># 获取label</span></span><br><span class=\"line\"><span class=\"comment\"># X, y = load_iris(return_X_y=True)</span></span><br><span class=\"line\"><span class=\"comment\"># print(X.shape, y.shape, type(X))</span></span><br></pre></td></tr></table></figure>\n\n<a id=\"more\"></a>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br><span class=\"line\">165</span><br><span class=\"line\">166</span><br><span class=\"line\">167</span><br><span class=\"line\">168</span><br><span class=\"line\">169</span><br><span class=\"line\">170</span><br><span class=\"line\">171</span><br><span class=\"line\">172</span><br><span class=\"line\">173</span><br><span class=\"line\">174</span><br><span class=\"line\">175</span><br><span class=\"line\">176</span><br><span class=\"line\">177</span><br><span class=\"line\">178</span><br><span class=\"line\">179</span><br><span class=\"line\">180</span><br><span class=\"line\">181</span><br><span class=\"line\">182</span><br><span class=\"line\">183</span><br><span class=\"line\">184</span><br><span class=\"line\">185</span><br><span class=\"line\">186</span><br><span class=\"line\">187</span><br><span class=\"line\">188</span><br><span class=\"line\">189</span><br><span class=\"line\">190</span><br><span class=\"line\">191</span><br><span class=\"line\">192</span><br><span class=\"line\">193</span><br><span class=\"line\">194</span><br><span class=\"line\">195</span><br><span class=\"line\">196</span><br><span class=\"line\">197</span><br><span class=\"line\">198</span><br><span class=\"line\">199</span><br><span class=\"line\">200</span><br><span class=\"line\">201</span><br><span class=\"line\">202</span><br><span class=\"line\">203</span><br><span class=\"line\">204</span><br><span class=\"line\">205</span><br><span class=\"line\">206</span><br><span class=\"line\">207</span><br><span class=\"line\">208</span><br><span class=\"line\">209</span><br><span class=\"line\">210</span><br><span class=\"line\">211</span><br><span class=\"line\">212</span><br><span class=\"line\">213</span><br><span class=\"line\">214</span><br><span class=\"line\">215</span><br><span class=\"line\">216</span><br><span class=\"line\">217</span><br><span class=\"line\">218</span><br><span class=\"line\">219</span><br><span class=\"line\">220</span><br><span class=\"line\">221</span><br><span class=\"line\">222</span><br><span class=\"line\">223</span><br><span class=\"line\">224</span><br><span class=\"line\">225</span><br><span class=\"line\">226</span><br><span class=\"line\">227</span><br><span class=\"line\">228</span><br><span class=\"line\">229</span><br><span class=\"line\">230</span><br><span class=\"line\">231</span><br><span class=\"line\">232</span><br><span class=\"line\">233</span><br><span class=\"line\">234</span><br><span class=\"line\">235</span><br><span class=\"line\">236</span><br><span class=\"line\">237</span><br><span class=\"line\">238</span><br><span class=\"line\">239</span><br><span class=\"line\">240</span><br><span class=\"line\">241</span><br><span class=\"line\">242</span><br><span class=\"line\">243</span><br><span class=\"line\">244</span><br><span class=\"line\">245</span><br><span class=\"line\">246</span><br><span class=\"line\">247</span><br><span class=\"line\">248</span><br><span class=\"line\">249</span><br><span class=\"line\">250</span><br><span class=\"line\">251</span><br><span class=\"line\">252</span><br><span class=\"line\">253</span><br><span class=\"line\">254</span><br><span class=\"line\">255</span><br><span class=\"line\">256</span><br><span class=\"line\">257</span><br><span class=\"line\">258</span><br><span class=\"line\">259</span><br><span class=\"line\">260</span><br><span class=\"line\">261</span><br><span class=\"line\">262</span><br><span class=\"line\">263</span><br><span class=\"line\">264</span><br><span class=\"line\">265</span><br><span class=\"line\">266</span><br><span class=\"line\">267</span><br><span class=\"line\">268</span><br><span class=\"line\">269</span><br><span class=\"line\">270</span><br><span class=\"line\">271</span><br><span class=\"line\">272</span><br><span class=\"line\">273</span><br><span class=\"line\">274</span><br><span class=\"line\">275</span><br><span class=\"line\">276</span><br><span class=\"line\">277</span><br><span class=\"line\">278</span><br><span class=\"line\">279</span><br><span class=\"line\">280</span><br><span class=\"line\">281</span><br><span class=\"line\">282</span><br><span class=\"line\">283</span><br><span class=\"line\">284</span><br><span class=\"line\">285</span><br><span class=\"line\">286</span><br><span class=\"line\">287</span><br><span class=\"line\">288</span><br><span class=\"line\">289</span><br><span class=\"line\">290</span><br><span class=\"line\">291</span><br><span class=\"line\">292</span><br><span class=\"line\">293</span><br><span class=\"line\">294</span><br><span class=\"line\">295</span><br><span class=\"line\">296</span><br><span class=\"line\">297</span><br><span class=\"line\">298</span><br><span class=\"line\">299</span><br><span class=\"line\">300</span><br><span class=\"line\">301</span><br><span class=\"line\">302</span><br><span class=\"line\">303</span><br><span class=\"line\">304</span><br><span class=\"line\">305</span><br><span class=\"line\">306</span><br><span class=\"line\">307</span><br><span class=\"line\">308</span><br><span class=\"line\">309</span><br><span class=\"line\">310</span><br><span class=\"line\">311</span><br><span class=\"line\">312</span><br><span class=\"line\">313</span><br><span class=\"line\">314</span><br><span class=\"line\">315</span><br><span class=\"line\">316</span><br><span class=\"line\">317</span><br><span class=\"line\">318</span><br><span class=\"line\">319</span><br><span class=\"line\">320</span><br><span class=\"line\">321</span><br><span class=\"line\">322</span><br><span class=\"line\">323</span><br><span class=\"line\">324</span><br><span class=\"line\">325</span><br><span class=\"line\">326</span><br><span class=\"line\">327</span><br><span class=\"line\">328</span><br><span class=\"line\">329</span><br><span class=\"line\">330</span><br><span class=\"line\">331</span><br><span class=\"line\">332</span><br><span class=\"line\">333</span><br><span class=\"line\">334</span><br><span class=\"line\">335</span><br><span class=\"line\">336</span><br><span class=\"line\">337</span><br><span class=\"line\">338</span><br><span class=\"line\">339</span><br><span class=\"line\">340</span><br><span class=\"line\">341</span><br><span class=\"line\">342</span><br><span class=\"line\">343</span><br><span class=\"line\">344</span><br><span class=\"line\">345</span><br><span class=\"line\">346</span><br><span class=\"line\">347</span><br><span class=\"line\">348</span><br><span class=\"line\">349</span><br><span class=\"line\">350</span><br><span class=\"line\">351</span><br><span class=\"line\">352</span><br><span class=\"line\">353</span><br><span class=\"line\">354</span><br><span class=\"line\">355</span><br><span class=\"line\">356</span><br><span class=\"line\">357</span><br><span class=\"line\">358</span><br><span class=\"line\">359</span><br><span class=\"line\">360</span><br><span class=\"line\">361</span><br><span class=\"line\">362</span><br><span class=\"line\">363</span><br><span class=\"line\">364</span><br><span class=\"line\">365</span><br><span class=\"line\">366</span><br><span class=\"line\">367</span><br><span class=\"line\">368</span><br><span class=\"line\">369</span><br><span class=\"line\">370</span><br><span class=\"line\">371</span><br><span class=\"line\">372</span><br><span class=\"line\">373</span><br><span class=\"line\">374</span><br><span class=\"line\">375</span><br><span class=\"line\">376</span><br><span class=\"line\">377</span><br><span class=\"line\">378</span><br><span class=\"line\">379</span><br><span class=\"line\">380</span><br><span class=\"line\">381</span><br><span class=\"line\">382</span><br><span class=\"line\">383</span><br><span class=\"line\">384</span><br><span class=\"line\">385</span><br><span class=\"line\">386</span><br><span class=\"line\">387</span><br><span class=\"line\">388</span><br><span class=\"line\">389</span><br><span class=\"line\">390</span><br><span class=\"line\">391</span><br><span class=\"line\">392</span><br><span class=\"line\">393</span><br><span class=\"line\">394</span><br><span class=\"line\">395</span><br><span class=\"line\">396</span><br><span class=\"line\">397</span><br><span class=\"line\">398</span><br><span class=\"line\">399</span><br><span class=\"line\">400</span><br><span class=\"line\">401</span><br><span class=\"line\">402</span><br><span class=\"line\">403</span><br><span class=\"line\">404</span><br><span class=\"line\">405</span><br><span class=\"line\">406</span><br><span class=\"line\">407</span><br><span class=\"line\">408</span><br><span class=\"line\">409</span><br><span class=\"line\">410</span><br><span class=\"line\">411</span><br><span class=\"line\">412</span><br><span class=\"line\">413</span><br><span class=\"line\">414</span><br><span class=\"line\">415</span><br><span class=\"line\">416</span><br><span class=\"line\">417</span><br><span class=\"line\">418</span><br><span class=\"line\">419</span><br><span class=\"line\">420</span><br><span class=\"line\">421</span><br><span class=\"line\">422</span><br><span class=\"line\">423</span><br><span class=\"line\">424</span><br><span class=\"line\">425</span><br><span class=\"line\">426</span><br><span class=\"line\">427</span><br><span class=\"line\">428</span><br><span class=\"line\">429</span><br><span class=\"line\">430</span><br><span class=\"line\">431</span><br><span class=\"line\">432</span><br><span class=\"line\">433</span><br><span class=\"line\">434</span><br><span class=\"line\">435</span><br><span class=\"line\">436</span><br><span class=\"line\">437</span><br><span class=\"line\">438</span><br><span class=\"line\">439</span><br><span class=\"line\">440</span><br><span class=\"line\">441</span><br><span class=\"line\">442</span><br><span class=\"line\">443</span><br><span class=\"line\">444</span><br><span class=\"line\">445</span><br><span class=\"line\">446</span><br><span class=\"line\">447</span><br><span class=\"line\">448</span><br><span class=\"line\">449</span><br><span class=\"line\">450</span><br><span class=\"line\">451</span><br><span class=\"line\">452</span><br><span class=\"line\">453</span><br><span class=\"line\">454</span><br><span class=\"line\">455</span><br><span class=\"line\">456</span><br><span class=\"line\">457</span><br><span class=\"line\">458</span><br><span class=\"line\">459</span><br><span class=\"line\">460</span><br><span class=\"line\">461</span><br><span class=\"line\">462</span><br><span class=\"line\">463</span><br><span class=\"line\">464</span><br><span class=\"line\">465</span><br><span class=\"line\">466</span><br><span class=\"line\">467</span><br><span class=\"line\">468</span><br><span class=\"line\">469</span><br><span class=\"line\">470</span><br><span class=\"line\">471</span><br><span class=\"line\">472</span><br><span class=\"line\">473</span><br><span class=\"line\">474</span><br><span class=\"line\">475</span><br><span class=\"line\">476</span><br><span class=\"line\">477</span><br><span class=\"line\">478</span><br><span class=\"line\">479</span><br><span class=\"line\">480</span><br><span class=\"line\">481</span><br><span class=\"line\">482</span><br><span class=\"line\">483</span><br><span class=\"line\">484</span><br><span class=\"line\">485</span><br><span class=\"line\">486</span><br><span class=\"line\">487</span><br><span class=\"line\">488</span><br><span class=\"line\">489</span><br><span class=\"line\">490</span><br><span class=\"line\">491</span><br><span class=\"line\">492</span><br><span class=\"line\">493</span><br><span class=\"line\">494</span><br><span class=\"line\">495</span><br><span class=\"line\">496</span><br><span class=\"line\">497</span><br><span class=\"line\">498</span><br><span class=\"line\">499</span><br><span class=\"line\">500</span><br><span class=\"line\">501</span><br><span class=\"line\">502</span><br><span class=\"line\">503</span><br><span class=\"line\">504</span><br><span class=\"line\">505</span><br><span class=\"line\">506</span><br><span class=\"line\">507</span><br><span class=\"line\">508</span><br><span class=\"line\">509</span><br><span class=\"line\">510</span><br><span class=\"line\">511</span><br><span class=\"line\">512</span><br><span class=\"line\">513</span><br><span class=\"line\">514</span><br><span class=\"line\">515</span><br><span class=\"line\">516</span><br><span class=\"line\">517</span><br><span class=\"line\">518</span><br><span class=\"line\">519</span><br><span class=\"line\">520</span><br><span class=\"line\">521</span><br><span class=\"line\">522</span><br><span class=\"line\">523</span><br><span class=\"line\">524</span><br><span class=\"line\">525</span><br><span class=\"line\">526</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 加载手写数字数据集</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\">digits = load_digits()</span><br><span class=\"line\">digits</span><br><span class=\"line\">print(digits.data.shape)</span><br><span class=\"line\">print(digits.target.shape)</span><br><span class=\"line\">print(digits.images.shape)</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_digits</span><br><span class=\"line\">digits = load_digits()</span><br><span class=\"line\">plt.matshow(digits.images[<span class=\"number\">10</span>])</span><br><span class=\"line\">plt.show</span><br><span class=\"line\"><span class=\"comment\"># 创建数据集</span></span><br><span class=\"line\"><span class=\"comment\"># 除了使用自带的数据集，还可以自己去常见训练样本</span></span><br><span class=\"line\"><span class=\"comment\"># https://scikit-learn.org/stable/datasets/</span></span><br><span class=\"line\"><span class=\"comment\"># 下面我们拿分类问题的样本生成器举例子：</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"><span class=\"comment\"># n_samples：指定样本数</span></span><br><span class=\"line\"><span class=\"comment\"># n_features：指定特征数</span></span><br><span class=\"line\"><span class=\"comment\"># n_classes：指定几分类</span></span><br><span class=\"line\"><span class=\"comment\"># random_state：随机种子，使得随机状可重</span></span><br><span class=\"line\">X,y = make_classification(n_samples = <span class=\"number\">6</span>, n_features = <span class=\"number\">5</span>,n_informative = <span class=\"number\">2</span>,</span><br><span class=\"line\">                         n_redundant = <span class=\"number\">2</span>,n_classes = <span class=\"number\">2</span>,n_clusters_per_class = <span class=\"number\">2</span>,</span><br><span class=\"line\">                         scale = <span class=\"number\">1.0</span>, random_state = <span class=\"number\">20</span>)</span><br><span class=\"line\">                         <span class=\"keyword\">for</span> x_,y_ <span class=\"keyword\">in</span> zip(X,y):</span><br><span class=\"line\">print(y_,end = <span class=\"string\">':'</span>)</span><br><span class=\"line\">print(x_)</span><br><span class=\"line\"><span class=\"comment\"># # 用sklearn.datasets.make_blobs来生成数据</span></span><br><span class=\"line\"><span class=\"comment\"># scikit中的make_blobs方法常被用来生成聚类算法的测试数据，</span></span><br><span class=\"line\"><span class=\"comment\"># 直观地说，make_blobs会根据用户指定的特征数量，</span></span><br><span class=\"line\"><span class=\"comment\"># 中心点数量，范围等来生成几类数据，这些数据可用于测试聚类算法的效果。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets.samples_generator <span class=\"keyword\">import</span> make_blobs</span><br><span class=\"line\">X = make_blobs(n_samples = <span class=\"number\">100</span>,n_features = <span class=\"number\">2</span>,centers = <span class=\"number\">3</span>,</span><br><span class=\"line\">                cluster_std = <span class=\"number\">1.0</span>,center_box = (<span class=\"number\">-10.0</span>,<span class=\"number\">10.0</span>),</span><br><span class=\"line\">                shuffle = <span class=\"literal\">True</span>,random_state = <span class=\"literal\">None</span>)</span><br><span class=\"line\">print(X)</span><br><span class=\"line\"><span class=\"comment\"># 例子（生成三类数据用于聚类）</span></span><br><span class=\"line\"><span class=\"comment\"># (100个样本，每个样本2个特征）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\">data,label = make_blobs(n_samples = <span class=\"number\">100</span>,n_features = <span class=\"number\">2</span>,</span><br><span class=\"line\">                       centers= <span class=\"number\">5</span>)</span><br><span class=\"line\">label</span><br><span class=\"line\">plt.scatter(data[:,<span class=\"number\">0</span>],data[:,<span class=\"number\">1</span>],c = label)</span><br><span class=\"line\"><span class=\"comment\">#  为每个类别设置不同的方差，</span></span><br><span class=\"line\"><span class=\"comment\"># 只需要在上述代码中加入cluster_std参数即可</span></span><br><span class=\"line\">data,target = make_blobs(n_samples = <span class=\"number\">100</span>,n_features = <span class=\"number\">2</span>,centers = <span class=\"number\">3</span>,</span><br><span class=\"line\">                        cluster_std = [<span class=\"number\">1.0</span>,<span class=\"number\">2.0</span>,<span class=\"number\">3.0</span>])</span><br><span class=\"line\">data</span><br><span class=\"line\"><span class=\"comment\">#   用sklearn.datasets.make_classification来生成数据</span></span><br><span class=\"line\">a = make_classification(n_samples=<span class=\"number\">100</span>,n_features = <span class=\"number\">20</span>,</span><br><span class=\"line\">                                    n_informative = <span class=\"number\">2</span>,n_redundant = <span class=\"number\">2</span>,n_repeated =<span class=\"number\">0</span>,</span><br><span class=\"line\">                                    n_classes = <span class=\"number\">2</span>, n_clusters_per_class = <span class=\"number\">2</span>,weights = <span class=\"literal\">None</span>,</span><br><span class=\"line\">                                    flip_y = <span class=\"number\">0.01</span>,class_sep = <span class=\"number\">1.0</span>,hypercube = <span class=\"literal\">True</span>,</span><br><span class=\"line\">                                    shift = <span class=\"number\">0.0</span>,scale = <span class=\"number\">1.0</span>,shuffle = <span class=\"literal\">True</span>, random_state = <span class=\"literal\">None</span>)</span><br><span class=\"line\"><span class=\"comment\"># n_features :特征个数= n_informative（） + n_redundant + n_repeated</span></span><br><span class=\"line\"><span class=\"comment\"># n_informative：多信息特征的个数</span></span><br><span class=\"line\"><span class=\"comment\"># n_redundant：冗余信息，informative特征的随机线性组合</span></span><br><span class=\"line\"><span class=\"comment\"># n_repeated ：重复信息，随机提取n_informative和n_redundant 特征</span></span><br><span class=\"line\"><span class=\"comment\"># n_classes：分类类别</span></span><br><span class=\"line\"><span class=\"comment\"># n_clusters_per_class ：某一个类别是由几个cluster构成的</span></span><br><span class=\"line\"><span class=\"comment\">#  用sklearn.datasets.make_gaussian</span></span><br><span class=\"line\"><span class=\"comment\"># 和make_hastie_10_2来生成数据</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_gaussian_quantiles</span><br><span class=\"line\">make_gaussian_quantiles(mean=<span class=\"literal\">None</span>, cov=<span class=\"number\">1.0</span>, n_samples=<span class=\"number\">100</span>,</span><br><span class=\"line\">                         n_features=<span class=\"number\">2</span>, n_classes=<span class=\"number\">3</span>,shuffle=<span class=\"literal\">True</span>, random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\"><span class=\"comment\"># 利用高斯分位点区分不同数据</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_hastie_10_2</span><br><span class=\"line\">make_hastie_10_2(n_samples=<span class=\"number\">12000</span>, random_state=<span class=\"literal\">None</span>)</span><br><span class=\"line\"><span class=\"comment\"># 利用Hastie算法，生成二分类数据</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_classification</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_blobs</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_gaussian_quantiles</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_hastie_10_2</span><br><span class=\"line\">plt.figure(figsize = (<span class=\"number\">8</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">plt.subplots_adjust(bottom=<span class=\"number\">.05</span>, top=<span class=\"number\">.9</span>, left=<span class=\"number\">.05</span>, right=<span class=\"number\">.95</span>)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">421</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"One informative feature, one cluster per class\"</span>, fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X1, Y1 = make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">2</span>, n_redundant=<span class=\"number\">0</span>, n_informative=<span class=\"number\">1</span>,</span><br><span class=\"line\">                             n_clusters_per_class=<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.scatter(X1[:, <span class=\"number\">0</span>], X1[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y1)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">422</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Two informative features, one cluster per class\"</span>, fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X1, Y1 = make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">2</span>, n_redundant=<span class=\"number\">0</span>, n_informative=<span class=\"number\">2</span>,</span><br><span class=\"line\">                             n_clusters_per_class=<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.scatter(X1[:, <span class=\"number\">0</span>], X1[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y1)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">423</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Two informative features, two clusters per class\"</span>, fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X2, Y2 = make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">2</span>, n_redundant=<span class=\"number\">0</span>, n_informative=<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.scatter(X2[:, <span class=\"number\">0</span>], X2[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y2)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">424</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Multi-class, two informative features, one cluster\"</span>,</span><br><span class=\"line\">          fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X1, Y1 = make_classification(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">2</span>, n_redundant=<span class=\"number\">0</span>, n_informative=<span class=\"number\">2</span>,</span><br><span class=\"line\">                             n_clusters_per_class=<span class=\"number\">1</span>, n_classes=<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.scatter(X1[:, <span class=\"number\">0</span>], X1[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y1)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">425</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Three blobs\"</span>, fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X1, Y1 = make_blobs(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">2</span>, centers=<span class=\"number\">3</span>)</span><br><span class=\"line\">plt.scatter(X1[:, <span class=\"number\">0</span>], X1[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y1)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">426</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"Gaussian divided into four quantiles\"</span>, fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X1, Y1 = make_gaussian_quantiles(n_samples=<span class=\"number\">1000</span>,n_features=<span class=\"number\">2</span>, n_classes=<span class=\"number\">4</span>)</span><br><span class=\"line\">plt.scatter(X1[:, <span class=\"number\">0</span>], X1[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y1)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">427</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">\"hastie data \"</span>, fontsize=<span class=\"string\">'small'</span>)</span><br><span class=\"line\">X1, Y1 = make_hastie_10_2(n_samples=<span class=\"number\">1000</span>)</span><br><span class=\"line\">plt.scatter(X1[:, <span class=\"number\">0</span>], X1[:, <span class=\"number\">1</span>], marker=<span class=\"string\">'o'</span>, c=Y1)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\"># 用sklearn.datasets.make_circles和</span></span><br><span class=\"line\"><span class=\"comment\"># make_moons来生成数据</span></span><br><span class=\"line\"><span class=\"comment\"># 生成环线数据</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_circles</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> make_moons</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\">fig=plt.figure(<span class=\"number\">2</span>)</span><br><span class=\"line\">x1,y1=make_circles(n_samples=<span class=\"number\">1000</span>,factor=<span class=\"number\">0.5</span>,noise=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.subplot(<span class=\"number\">121</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'make_circles function example'</span>)</span><br><span class=\"line\">plt.scatter(x1[:,<span class=\"number\">0</span>],x1[:,<span class=\"number\">1</span>],marker=<span class=\"string\">'o'</span>,c=y1)</span><br><span class=\"line\">  </span><br><span class=\"line\">plt.subplot(<span class=\"number\">122</span>)</span><br><span class=\"line\">x1,y1=make_moons(n_samples=<span class=\"number\">1000</span>,noise=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">'make_moons function example'</span>)</span><br><span class=\"line\">plt.scatter(x1[:,<span class=\"number\">0</span>],x1[:,<span class=\"number\">1</span>],marker=<span class=\"string\">'o'</span>,c=y1)</span><br><span class=\"line\">plt.show()</span><br><span class=\"line\"><span class=\"comment\"># 2\\数据预处理</span></span><br><span class=\"line\"><span class=\"comment\"># 数据预处理阶段是机器学习中不可缺少的一环，</span></span><br><span class=\"line\"><span class=\"comment\"># 它会使得数据更加有效的被模型或者评估器识别。</span></span><br><span class=\"line\"><span class=\"comment\"># 下面我们来看一下sklearn中有哪些平时我们常用的函数：</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\"><span class=\"comment\"># 为了使得训练数据的标准化规则与测试数据的标准化规则同步，</span></span><br><span class=\"line\"><span class=\"comment\"># preprocessing中提供了很多的Scaler：</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># StandardScaler</span></span><br><span class=\"line\"><span class=\"comment\"># MaxAbsScaler</span></span><br><span class=\"line\"><span class=\"comment\"># MinMaxScaler</span></span><br><span class=\"line\"><span class=\"comment\"># RobustScaler</span></span><br><span class=\"line\"><span class=\"comment\"># Normalizer</span></span><br><span class=\"line\"><span class=\"comment\"># 等其他预处理操作</span></span><br><span class=\"line\"><span class=\"comment\"># 对应的有直接的函数使用：</span></span><br><span class=\"line\"><span class=\"comment\"># scale()，maxabs_scale()，minmax_scale()，</span></span><br><span class=\"line\"><span class=\"comment\"># robust_scale()，normaizer（）</span></span><br><span class=\"line\"><span class=\"comment\"># sklearn.preprocessing.scale(X)</span></span><br><span class=\"line\"><span class=\"comment\"># 2.1数据标准化</span></span><br><span class=\"line\"><span class=\"comment\"># 标准化：在机器学习中，我们可能要处理不同种类的资料，</span></span><br><span class=\"line\"><span class=\"comment\"># 例如，音讯和图片上的像素值，这些资料可能是高纬度的，</span></span><br><span class=\"line\"><span class=\"comment\"># 资料标准化后会使得每个特征中的数值平均变为0</span></span><br><span class=\"line\"><span class=\"comment\"># （将每个特征的值都减掉原始资料中该特征的平均），标准差变为1，</span></span><br><span class=\"line\"><span class=\"comment\"># 这个方法被广泛的使用在许多机器学习算法中</span></span><br><span class=\"line\"><span class=\"comment\"># （例如：支持向量机，逻辑回归和类神经网络）。</span></span><br><span class=\"line\"><span class=\"comment\"># StandardScaler计算训练集的平均值和标准差，</span></span><br><span class=\"line\"><span class=\"comment\"># 以便测试数据及使用相同的变换。</span></span><br><span class=\"line\"><span class=\"comment\"># 变换后各维特征有0均值，单位方差，也叫z-score规范化（零均值规范化），</span></span><br><span class=\"line\"><span class=\"comment\"># 计算方式是将特征值减去均值，除以标准差。</span></span><br><span class=\"line\"><span class=\"comment\"># fit</span></span><br><span class=\"line\"><span class=\"comment\"># 用于计算训练数据的均值和方差，</span></span><br><span class=\"line\"><span class=\"comment\"># 后面就会用均值和方差来转换训练数据</span></span><br><span class=\"line\"><span class=\"comment\"># fit_transform</span></span><br><span class=\"line\"><span class=\"comment\"># 不仅计算训练数据的均值和方差，</span></span><br><span class=\"line\"><span class=\"comment\"># 还会基于计算出来的均值和方差来转换训练数据，</span></span><br><span class=\"line\"><span class=\"comment\"># 从而把数据转化成标准的正态分布。</span></span><br><span class=\"line\"><span class=\"comment\"># transform</span></span><br><span class=\"line\"><span class=\"comment\"># 很显然，它只是进行转换，只是把训练数据转换成标准的正态分布。</span></span><br><span class=\"line\"><span class=\"comment\"># （一般会把train和test集放在一起做标准化，</span></span><br><span class=\"line\"><span class=\"comment\"># 或者在train集上做标准化后，用同样的标准化器去标准化test集，</span></span><br><span class=\"line\"><span class=\"comment\"># 此时可以使用scaler)。</span></span><br><span class=\"line\">data = [[<span class=\"number\">0</span>,<span class=\"number\">0</span>],[<span class=\"number\">0</span>,<span class=\"number\">0</span>],[<span class=\"number\">1</span>,<span class=\"number\">1</span>],[<span class=\"number\">1</span>,<span class=\"number\">1</span>]]</span><br><span class=\"line\"><span class=\"comment\"># 1.给予mean和std的标准化</span></span><br><span class=\"line\"><span class=\"comment\"># scaler = preprocessing.StandardScaler().fit(train_data)</span></span><br><span class=\"line\"><span class=\"comment\"># scaler.transform(train_data)</span></span><br><span class=\"line\"><span class=\"comment\"># scaler.transform(tesst_data)</span></span><br><span class=\"line\"><span class=\"comment\"># # 一般来说先试用fit</span></span><br><span class=\"line\"><span class=\"comment\"># scaler = preprocessing.StandardScaler().fit(X)</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步可以计算得到scaler，</span></span><br><span class=\"line\"><span class=\"comment\"># scaler里面存的有计算出来的均值和方差。</span></span><br><span class=\"line\"><span class=\"comment\"># 再使用transform</span></span><br><span class=\"line\"><span class=\"comment\"># scaler.transform(X)</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步再用scaler中的均值和方差来转换X，使X标准化。</span></span><br><span class=\"line\"><span class=\"comment\"># 最后，在预测的时候，也要对数据做同样的标准化处理，</span></span><br><span class=\"line\"><span class=\"comment\"># 即也要用上面的scaler中的均值和方差来对预测时候的特征进行标准化。</span></span><br><span class=\"line\"><span class=\"comment\"># 注意：测试数据和预测数据的标准化的方式要和训练数据标准化的方式一样，</span></span><br><span class=\"line\"><span class=\"comment\"># 必须使用同一个scaler来进行transform</span></span><br><span class=\"line\"><span class=\"comment\"># 2.2 最小-最大规范化</span></span><br><span class=\"line\"><span class=\"comment\"># 最小最大规范化对原始数据进行线性变换，变换到[0,1]区间</span></span><br><span class=\"line\"><span class=\"comment\"># （也可以是其他固定最小最大值的区间）。</span></span><br><span class=\"line\"><span class=\"comment\"># 例子：将每个特征值归一化到一个固定范围</span></span><br><span class=\"line\">scaler = preprocessing.MinMaxScaler(feature_range=(<span class=\"number\">0</span>, <span class=\"number\">1</span>)).fit(train_data)</span><br><span class=\"line\">scaler.transform(train_data)</span><br><span class=\"line\">scaler.transform(test_data)</span><br><span class=\"line\"><span class=\"comment\"># #feature_range: 定义归一化范围，注用（）括起来</span></span><br><span class=\"line\"><span class=\"comment\"># 2.3 正则化（normalize）</span></span><br><span class=\"line\"><span class=\"comment\"># 当你想要计算两个样本的相似度时必不可少的一个操作，就是正则化。</span></span><br><span class=\"line\"><span class=\"comment\"># 其思想是：首先求出样本的p范数，然后该样本的所有元素都要除以该范数，</span></span><br><span class=\"line\"><span class=\"comment\"># 这样最终使得每个样本的范数都是1。</span></span><br><span class=\"line\"><span class=\"comment\"># 与2.2规范化不同，规范化（Normalization）是将不同变化范围的值映射到相同的固定范围，</span></span><br><span class=\"line\"><span class=\"comment\"># 常见的是[0,1]，也成为归一化。</span></span><br><span class=\"line\">X_normalized = preprocessing.normalize(X, norm=<span class=\"string\">'l2'</span>)</span><br><span class=\"line\"><span class=\"comment\"># one-hot编码</span></span><br><span class=\"line\"><span class=\"comment\"># one-hot编码是一种对离散特征值的编码方式，</span></span><br><span class=\"line\"><span class=\"comment\"># 在LR模型中常用到，用于给线性模型增加非线性能力。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> preprocessing</span><br><span class=\"line\">data = [[<span class=\"number\">0</span>, <span class=\"number\">0</span>, <span class=\"number\">3</span>], [<span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>], [<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>], [<span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">2</span>]]</span><br><span class=\"line\">encoder = preprocessing.OneHotEncoder().fit(data)</span><br><span class=\"line\">enc.transform(data).toarray()</span><br><span class=\"line\"><span class=\"comment\"># 2.5 特征二值化（Binarization）</span></span><br><span class=\"line\"><span class=\"comment\"># 给定阈值，将特征转换为0/1.</span></span><br><span class=\"line\"><span class=\"comment\"># binarizer = sklearn.preprocessing.Binarizer(threshold=1.1)</span></span><br><span class=\"line\"><span class=\"comment\"># binarizer.transform(X)</span></span><br><span class=\"line\"><span class=\"comment\"># 2.6 类别特征编码</span></span><br><span class=\"line\"><span class=\"comment\"># 有时候特征时类别型的，而一些算法的输入必须是数值型，</span></span><br><span class=\"line\"><span class=\"comment\"># 此时需要对其编码，</span></span><br><span class=\"line\"><span class=\"comment\"># enc = preprocessing.OneHotEncoder()</span></span><br><span class=\"line\"><span class=\"comment\"># enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])</span></span><br><span class=\"line\"><span class=\"comment\"># enc.transform([[0, 1, 3]]).toarray()  </span></span><br><span class=\"line\"><span class=\"comment\"># array([[ 1., 0., 0., 1., 0., 0., 0., 0., 1.]])</span></span><br><span class=\"line\"><span class=\"comment\"># 第一维特征有两种值0和1，用两位去编码。</span></span><br><span class=\"line\"><span class=\"comment\"># 第二维用三位，第三维用四位。</span></span><br><span class=\"line\"><span class=\"comment\"># 2.7 标签编码（Label encoding）</span></span><br><span class=\"line\"><span class=\"comment\"># le = sklearn.preprocessing.LabelEncoder() </span></span><br><span class=\"line\"><span class=\"comment\"># le.fit([1, 2, 2, 6])</span></span><br><span class=\"line\"><span class=\"comment\"># le.transform([1, 1, 2, 6])  #array([0, 0, 1, 2])</span></span><br><span class=\"line\"><span class=\"comment\"># #非数值型转化为数值型</span></span><br><span class=\"line\"><span class=\"comment\"># le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])</span></span><br><span class=\"line\"><span class=\"comment\"># le.transform([\"tokyo\", \"tokyo\", \"paris\"])  #array([2, 2, 1])</span></span><br><span class=\"line\"><span class=\"comment\"># 3.数据集拆分</span></span><br><span class=\"line\"><span class=\"comment\"># 在得到训练数据集时，</span></span><br><span class=\"line\"><span class=\"comment\"># 通常我们经常会把训练数据进一步拆分成训练集和验证集，</span></span><br><span class=\"line\"><span class=\"comment\"># 这样有助于我们模型参数的选取。</span></span><br><span class=\"line\"><span class=\"comment\"># train_test_split是交叉验证中常用的函数，</span></span><br><span class=\"line\"><span class=\"comment\"># 功能是从样本中随机的按比例选取train data和testdata，形式为：</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.mode_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\">X_train,X_test,y_train,y_test = train_test_split(train_data,train_target,</span><br><span class=\"line\">                                                test_size=<span class=\"number\">0.4</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"><span class=\"comment\"># train_data：所要划分的样本特征集</span></span><br><span class=\"line\"><span class=\"comment\"># train_target：所要划分的样本结果</span></span><br><span class=\"line\"><span class=\"comment\"># test_size：样本占比(测试集样本数目与原始样本数目之比，默认：0.25)，</span></span><br><span class=\"line\"><span class=\"comment\"># 如果是整数的话就是样本的数量</span></span><br><span class=\"line\"><span class=\"comment\"># random_state：是随机数的种子。</span></span><br><span class=\"line\"><span class=\"comment\"># 随机数种子：其实就是该组随机数的编号，在需要重复试验的时候，保证得到一组一样的随机数。比如你每次都填1，其他参数一样的情况下你得到的随机数组是一样的。但填0或不填，每次都会不一样。</span></span><br><span class=\"line\"><span class=\"comment\"># 随机数的产生取决于种子，随机数和种子之间的关系遵从以下两个规则：</span></span><br><span class=\"line\"><span class=\"comment\"># 　　　　种子不同，产生不同的随机数</span></span><br><span class=\"line\"><span class=\"comment\"># 　　　　种子相同，即使实例不同也产生相同的随机数</span></span><br><span class=\"line\"><span class=\"comment\"># 4、定义模型</span></span><br><span class=\"line\"><span class=\"comment\"># 在这一步我们首先要分析自己数据的类型，</span></span><br><span class=\"line\"><span class=\"comment\"># 明白自己要用什么模型来做，然后我们就可以在sklearn中定义模型了，</span></span><br><span class=\"line\"><span class=\"comment\"># sklearn为所有模型提供了非常相似的接口，</span></span><br><span class=\"line\"><span class=\"comment\"># 这样使得我们可以更加快速的熟悉所有模型的用法，</span></span><br><span class=\"line\"><span class=\"comment\"># 在这之前，我们先来看看模型的常用属性和功能。</span></span><br><span class=\"line\"><span class=\"comment\"># # 拟合模型</span></span><br><span class=\"line\"><span class=\"comment\"># model.fit(X_train, y_train)</span></span><br><span class=\"line\"><span class=\"comment\"># # 模型预测</span></span><br><span class=\"line\"><span class=\"comment\"># model.predict(X_test)</span></span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"comment\"># # 获得这个模型的参数</span></span><br><span class=\"line\"><span class=\"comment\"># model.get_params()</span></span><br><span class=\"line\"><span class=\"comment\"># # 为模型进行打分</span></span><br><span class=\"line\"><span class=\"comment\"># model.score(data_X, data_y) # 线性回归：R square； 分类问题： acc</span></span><br><span class=\"line\"><span class=\"comment\"># 4.1线性回归</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LinearRegression</span><br><span class=\"line\"><span class=\"comment\"># 定义线性回归模型</span></span><br><span class=\"line\">model = LinearRegression(fit_intercept=<span class=\"literal\">True</span>, normalize=<span class=\"literal\">False</span>,</span><br><span class=\"line\">    copy_X=<span class=\"literal\">True</span>, n_jobs=<span class=\"number\">1</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    fit_intercept：是否计算截距。False-模型没有截距</span></span><br><span class=\"line\"><span class=\"string\">    normalize： 当fit_intercept设置为False时，该参数将被忽略。 如果为真，</span></span><br><span class=\"line\"><span class=\"string\">则回归前的回归系数X将通过减去平均值并除以l2-范数而归一化。</span></span><br><span class=\"line\"><span class=\"string\">     n_jobs：指定线程数</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\"># 4.2 逻辑回归LR</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"comment\"># 定义逻辑回归模型</span></span><br><span class=\"line\">model = LogisticRegression(penalty=’l2’, dual=<span class=\"literal\">False</span>, tol=<span class=\"number\">0.0001</span>, C=<span class=\"number\">1.0</span>,</span><br><span class=\"line\">    fit_intercept=<span class=\"literal\">True</span>, intercept_scaling=<span class=\"number\">1</span>, class_weight=<span class=\"literal\">None</span>,</span><br><span class=\"line\">    random_state=<span class=\"literal\">None</span>, solver=’liblinear’, max_iter=<span class=\"number\">100</span>, multi_class=’ovr’,</span><br><span class=\"line\">    verbose=<span class=\"number\">0</span>, warm_start=<span class=\"literal\">False</span>, n_jobs=<span class=\"number\">1</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\"><span class=\"string\">\"\"\"参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    penalty：使用指定正则化项（默认：l2）</span></span><br><span class=\"line\"><span class=\"string\">    dual: n_samples &gt; n_features取False（默认）</span></span><br><span class=\"line\"><span class=\"string\">    C：正则化强度的反，值越小正则化强度越大</span></span><br><span class=\"line\"><span class=\"string\">    n_jobs: 指定线程数</span></span><br><span class=\"line\"><span class=\"string\">    random_state：随机数生成器</span></span><br><span class=\"line\"><span class=\"string\">    fit_intercept: 是否需要常量</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\"># 4.3 朴素贝叶斯算法NB（Naive Bayes）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> naive_bayes</span><br><span class=\"line\">model = naive_bayes.GaussianNB() <span class=\"comment\"># 高斯贝叶斯</span></span><br><span class=\"line\">model = naive_bayes.MultinomialNB(alpha=<span class=\"number\">1.0</span>, fit_prior=<span class=\"literal\">True</span>, class_prior=<span class=\"literal\">None</span>)</span><br><span class=\"line\">model = naive_bayes.BernoulliNB(alpha=<span class=\"number\">1.0</span>, binarize=<span class=\"number\">0.0</span>, fit_prior=<span class=\"literal\">True</span>, class_prior=<span class=\"literal\">None</span>)</span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">文本分类问题常用MultinomialNB</span></span><br><span class=\"line\"><span class=\"string\">参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    alpha：平滑参数</span></span><br><span class=\"line\"><span class=\"string\">    fit_prior：是否要学习类的先验概率；false-使用统一的先验概率</span></span><br><span class=\"line\"><span class=\"string\">    class_prior: 是否指定类的先验概率；若指定则不能根据参数调整</span></span><br><span class=\"line\"><span class=\"string\">    binarize: 二值化的阈值，若为None，则假设输入由二进制向量组成</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\">#  4.4 决策树DT</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> tree</span><br><span class=\"line\">model = tree.DecisionTreeClassifier(criterion=’gini’, max_depth=<span class=\"literal\">None</span>,</span><br><span class=\"line\">    min_samples_split=<span class=\"number\">2</span>, min_samples_leaf=<span class=\"number\">1</span>, min_weight_fraction_leaf=<span class=\"number\">0.0</span>,</span><br><span class=\"line\">    max_features=<span class=\"literal\">None</span>, random_state=<span class=\"literal\">None</span>, max_leaf_nodes=<span class=\"literal\">None</span>,</span><br><span class=\"line\">    min_impurity_decrease=<span class=\"number\">0.0</span>, min_impurity_split=<span class=\"literal\">None</span>,</span><br><span class=\"line\">     class_weight=<span class=\"literal\">None</span>, presort=<span class=\"literal\">False</span>)</span><br><span class=\"line\"><span class=\"string\">\"\"\"参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    criterion ：特征选择准则gini/entropy</span></span><br><span class=\"line\"><span class=\"string\">    max_depth：树的最大深度，None-尽量下分</span></span><br><span class=\"line\"><span class=\"string\">    min_samples_split：分裂内部节点，所需要的最小样本树</span></span><br><span class=\"line\"><span class=\"string\">    min_samples_leaf：叶子节点所需要的最小样本数</span></span><br><span class=\"line\"><span class=\"string\">    max_features: 寻找最优分割点时的最大特征数</span></span><br><span class=\"line\"><span class=\"string\">    max_leaf_nodes：优先增长到最大叶子节点数</span></span><br><span class=\"line\"><span class=\"string\">    min_impurity_decrease：如果这种分离导致杂质的减少大于或等于这个值，则节点将被拆分。</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\"># 4.5 支持向量机SVM</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.svm <span class=\"keyword\">import</span> SVC</span><br><span class=\"line\">model = SVC(C=<span class=\"number\">1.0</span>, kernel=’rbf’, gamma=’auto’)</span><br><span class=\"line\"><span class=\"string\">\"\"\"参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    C：误差项的惩罚参数C</span></span><br><span class=\"line\"><span class=\"string\">    gamma: 核相关系数。浮点数，If gamma is ‘auto’ then 1/n_features will be used instead.</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\"># 4.6 k近邻算法KNN</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> neighbors</span><br><span class=\"line\"><span class=\"comment\">#定义kNN分类模型</span></span><br><span class=\"line\">model = neighbors.KNeighborsClassifier(n_neighbors=<span class=\"number\">5</span>, n_jobs=<span class=\"number\">1</span>) <span class=\"comment\"># 分类</span></span><br><span class=\"line\">model = neighbors.KNeighborsRegressor(n_neighbors=<span class=\"number\">5</span>, n_jobs=<span class=\"number\">1</span>) <span class=\"comment\"># 回归</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    n_neighbors： 使用邻居的数目</span></span><br><span class=\"line\"><span class=\"string\">    n_jobs：并行任务数</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\"># 4.7 多层感知器（神经网络）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.neural_network <span class=\"keyword\">import</span> MLPClassifier</span><br><span class=\"line\"><span class=\"comment\"># 定义多层感知机分类算法</span></span><br><span class=\"line\">model = MLPClassifier(activation=<span class=\"string\">'relu'</span>, solver=<span class=\"string\">'adam'</span>, alpha=<span class=\"number\">0.0001</span>)</span><br><span class=\"line\"><span class=\"string\">\"\"\"参数</span></span><br><span class=\"line\"><span class=\"string\">---</span></span><br><span class=\"line\"><span class=\"string\">    hidden_layer_sizes: 元祖</span></span><br><span class=\"line\"><span class=\"string\">    activation：激活函数</span></span><br><span class=\"line\"><span class=\"string\">    solver ：优化算法&#123;‘lbfgs’, ‘sgd’, ‘adam’&#125;</span></span><br><span class=\"line\"><span class=\"string\">    alpha：L2惩罚(正则化项)参数。</span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"comment\"># 5、模型评估与选择</span></span><br><span class=\"line\"><span class=\"comment\"># 评价指标针对不同的机器学习任务有不同的指标，</span></span><br><span class=\"line\"><span class=\"comment\"># 同一任务也有不同侧重点的评价指标。</span></span><br><span class=\"line\"><span class=\"comment\"># 以下方法，sklearn中都在sklearn.metrics类下，</span></span><br><span class=\"line\"><span class=\"comment\"># 务必记住那些指标适合分类，那些适合回归。</span></span><br><span class=\"line\"><span class=\"comment\"># https://www.geek-share.com/detail/2749032140.html</span></span><br><span class=\"line\"><span class=\"comment\"># 分类：accuracy(准确率)、f1、f1_micro、f1_macro（这两个用于多分类的f1_score）、precision(精确度)、recall(召回率)、roc_auc</span></span><br><span class=\"line\"><span class=\"comment\"># 回归：neg_mean_squared_error（MSE、均方误差）、r2</span></span><br><span class=\"line\"><span class=\"comment\"># 聚类：adjusted_rand_score、completeness_score等 </span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score  </span><br><span class=\"line\">cross_val_score(model, X, y=<span class=\"literal\">None</span>, scoring=<span class=\"literal\">None</span>, cv=<span class=\"literal\">None</span>, n_jobs=<span class=\"number\">1</span>)  </span><br><span class=\"line\"><span class=\"string\">\"\"\"参数  </span></span><br><span class=\"line\"><span class=\"string\">---  </span></span><br><span class=\"line\"><span class=\"string\">model：拟合数据的模型  </span></span><br><span class=\"line\"><span class=\"string\">cv ： k-fold  </span></span><br><span class=\"line\"><span class=\"string\">scoring: 打分参数-‘accuracy’、‘f1’、‘precision’、‘recall’ 、‘roc_auc’、'neg_log_loss'等等  </span></span><br><span class=\"line\"><span class=\"string\">\"\"\"</span>  </span><br><span class=\"line\"><span class=\"comment\">#交叉验证</span></span><br><span class=\"line\"><span class=\"comment\"># 6、保存模型</span></span><br><span class=\"line\"><span class=\"comment\"># 我们可以将我们训练好的model保存到本地，或者放到线上供用户使用，</span></span><br><span class=\"line\"><span class=\"comment\"># 那么如何保存训练好的model呢？主要有下面两种方式：</span></span><br><span class=\"line\"><span class=\"comment\"># 6.1 保存为pickle文件</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"comment\"># 保存模型</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">'model.pickle'</span>, <span class=\"string\">'wb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    pickle.dump(model, f)</span><br><span class=\"line\"><span class=\"comment\"># 读取模型</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> open(<span class=\"string\">'model.pickle'</span>, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> f:</span><br><span class=\"line\">    model = pickle.load(f)</span><br><span class=\"line\">model.predict(X_test)</span><br><span class=\"line\"><span class=\"comment\"># 6.2 sklearn自带方法joblib</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.externals <span class=\"keyword\">import</span> joblib</span><br><span class=\"line\"><span class=\"comment\"># 保存模型</span></span><br><span class=\"line\">joblib.dump(model, <span class=\"string\">'model.pickle'</span>)</span><br><span class=\"line\"><span class=\"comment\">#载入模型</span></span><br><span class=\"line\">model = joblib.load(<span class=\"string\">'model.pickle'</span>)</span><br><span class=\"line\"><span class=\"comment\"># 7，模型评分</span></span><br><span class=\"line\"><span class=\"comment\"># 1，模型的score方法：</span></span><br><span class=\"line\"><span class=\"comment\"># 最简单的模型评估方法就是调用模型自己的方法：</span></span><br><span class=\"line\">y_predict = knnClf.predict(x_test)</span><br><span class=\"line\">print(<span class=\"string\">\"score on the testdata:\"</span>,knnClf.score(x_test,y_test))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2，sklearn的指标函数：库提供的一些计算方法，</span></span><br><span class=\"line\"><span class=\"comment\">#    常用的有classification_report方法</span></span><br><span class=\"line\"><span class=\"comment\"># 3，sklearn也支持自己开发评价方法。</span></span><br><span class=\"line\"><span class=\"comment\"># 8，几种交叉验证（cross validation）方式的比较</span></span><br><span class=\"line\"><span class=\"comment\"># 　模型评价的目的：通过模型评价，我们知道当前训练模型的好坏，</span></span><br><span class=\"line\"><span class=\"comment\">#                    泛化能力如何？从而知道是否可以应用在解决问题上，</span></span><br><span class=\"line\"><span class=\"comment\">#                   如果不行，那又是那些出了问题？</span></span><br><span class=\"line\">train_test_split</span><br><span class=\"line\"><span class=\"comment\"># 　　在分类问题中，我们通常通过对训练集进行triain_test_split，</span></span><br><span class=\"line\"><span class=\"comment\">#     划分出train 和test两部分，其中train用来训练模型，test用来评估模型，</span></span><br><span class=\"line\"><span class=\"comment\">#     模型通过fit方法从train数据集中学习，</span></span><br><span class=\"line\"><span class=\"comment\">#     然后调用score方法在test集上进行评估，打分；</span></span><br><span class=\"line\"><span class=\"comment\">#     从分数上我们知道模型当前的训练水平如何。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_breast_cancer</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">import</span>  matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"> </span><br><span class=\"line\">cancer = load_breast_cancer()</span><br><span class=\"line\">X_train,X_test,y_train,y_test = train_test_split(cancer.data,cancer.target,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">logreg = LogisticRegression().fit(X_train,y_train)</span><br><span class=\"line\">print(<span class=\"string\">\"Test set score:&#123;:.2f&#125;\"</span>.format(logreg.score(X_test,y_test)))</span><br><span class=\"line\"><span class=\"comment\"># 结果：</span></span><br><span class=\"line\">Test set score:<span class=\"number\">0.96</span></span><br><span class=\"line\"><span class=\"comment\"># 然而这这方式只进行了一次划分，数据结果具有偶然性，</span></span><br><span class=\"line\"><span class=\"comment\"># 如果在某次划分中，训练集里全是容易学习的数据，</span></span><br><span class=\"line\"><span class=\"comment\"># 测试集里全是复杂的数据，这样的就会导致最终的结果不尽人意。</span></span><br><span class=\"line\">Standard Cross Validation</span><br><span class=\"line\"><span class=\"comment\"># 交叉验证我觉得讲的没有莫烦的好</span></span><br><span class=\"line\"><span class=\"comment\"># https://mofanpy.com/</span></span><br><span class=\"line\"><span class=\"comment\"># 针对上面通过train_test_split划分，</span></span><br><span class=\"line\"><span class=\"comment\"># 从而进行模型评估方式存在的弊端，提出Cross Validation交叉验证。</span></span><br><span class=\"line\"><span class=\"comment\"># Cross Validation：进行多次train_test_split划分；</span></span><br><span class=\"line\"><span class=\"comment\"># 每次划分时，在不同的数据集上进行训练，测试评估，从而得到一个评价结果；</span></span><br><span class=\"line\"><span class=\"comment\"># 如果是5折交叉验证，意思就是在原始数据集上，进行五次划分，</span></span><br><span class=\"line\"><span class=\"comment\"># 每次划分进行一次训练，评估，最后得到5次划分后的评估结果，</span></span><br><span class=\"line\"><span class=\"comment\"># 一般在这几次评估结果上取平均得到最后的评分，</span></span><br><span class=\"line\"><span class=\"comment\"># k-folf cross-validation ，其中K一般取5或10。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> cross_val_score</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_breast_cancer</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">import</span>  warnings</span><br><span class=\"line\"> </span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">'ignore'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">cancer = load_breast_cancer()</span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(</span><br><span class=\"line\">    cancer.data , cancer.target, random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">logreg = LogisticRegression()</span><br><span class=\"line\"><span class=\"comment\"># CV 默认是3折交叉验证，可以修改cv=5，变为5折交叉验证</span></span><br><span class=\"line\">scores = cross_val_score(logreg,cancer.data , cancer.target)</span><br><span class=\"line\"></span><br><span class=\"line\">print(<span class=\"string\">\"Cross validation scores:&#123;&#125;\"</span>.format(scores))</span><br><span class=\"line\">print(<span class=\"string\">\"Mean cross validation score:&#123;:2f&#125;\"</span>.format(scores.mean()))</span><br><span class=\"line\"><span class=\"comment\"># 交叉验证的优点：</span></span><br><span class=\"line\"><span class=\"comment\"># 原始采用的train_test_split方法，数据划分具有偶然性；</span></span><br><span class=\"line\"><span class=\"comment\"># 交叉验证通过多次划分，大大降低了这种由一次随机划分带来的偶然性，</span></span><br><span class=\"line\"><span class=\"comment\"># 同时通过多次划分，多次训练，模型也能遇到各种各样的数据，从而提高其泛化能力</span></span><br><span class=\"line\"><span class=\"comment\"># 与原始的train_test_split相比，对数据的使用效率更高，</span></span><br><span class=\"line\"><span class=\"comment\"># train_test_split，默认训练集，测试集比例为3:1，</span></span><br><span class=\"line\"><span class=\"comment\"># 而对交叉验证来说，如果是5折交叉验证，训练集比测试集为4:1；</span></span><br><span class=\"line\"><span class=\"comment\"># 10折交叉验证训练集比测试集为9:1.数据量越大，模型准确率越高！</span></span><br><span class=\"line\"><span class=\"comment\"># 交叉验证的缺点：</span></span><br><span class=\"line\"><span class=\"comment\"># 这种简答的交叉验证方式，每次划分时对数据进行均分，</span></span><br><span class=\"line\"><span class=\"comment\"># 设想一下，会不会存在一种情况：数据集有5类，抽取出来的也正好是按照类别划分的5类，</span></span><br><span class=\"line\"><span class=\"comment\"># 也就是说第一折全是0类，第二折全是1类，等等；</span></span><br><span class=\"line\"><span class=\"comment\"># 这样的结果就会导致，模型训练时，</span></span><br><span class=\"line\"><span class=\"comment\"># 没有学习到测试集中数据的特点，从而导致模型得分很低，甚至为0，</span></span><br><span class=\"line\"><span class=\"comment\"># 为避免这种情况，又出现了其他的各种交叉验证方式。</span></span><br><span class=\"line\"><span class=\"comment\"># Stratifid k-fold cross validation</span></span><br><span class=\"line\"><span class=\"comment\"># 分层交叉验证（Stratified k-fold cross validation）：</span></span><br><span class=\"line\"><span class=\"comment\"># 首先它属于交叉验证类型，</span></span><br><span class=\"line\"><span class=\"comment\"># 分层的意思是说在每一折中都保持着原始数据中各个类别的比例关系，</span></span><br><span class=\"line\"><span class=\"comment\"># 比如说：原始数据有3类，比例为1:2:1，</span></span><br><span class=\"line\"><span class=\"comment\"># 采用3折分层交叉验证，那么划分的3折中，</span></span><br><span class=\"line\"><span class=\"comment\"># 每一折中的数据类别保持着1:2:1的比例，这样的验证结果更加可信。</span></span><br><span class=\"line\"><span class=\"comment\"># 通常情况下，可以设置cv参数来控制几折，</span></span><br><span class=\"line\"><span class=\"comment\"># 但是我们希望对其划分等加以控制，</span></span><br><span class=\"line\"><span class=\"comment\"># 所以出现了KFold，KFold控制划分折，</span></span><br><span class=\"line\"><span class=\"comment\"># 可以控制划分折的数目，是否打乱顺序等，</span></span><br><span class=\"line\"><span class=\"comment\"># 可以赋值给cv，用来控制划分。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> StratifiedKFold ,cross_val_score</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"> </span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">'ignore'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">iris_data = load_iris()</span><br><span class=\"line\">logreg = LogisticRegression()</span><br><span class=\"line\">strKFold = StratifiedKFold(n_splits=<span class=\"number\">3</span>,shuffle=<span class=\"literal\">False</span>,random_state=<span class=\"number\">0</span>)</span><br><span class=\"line\">scores = cross_val_score(logreg,iris_data.data,iris_data.target,cv=strKFold)</span><br><span class=\"line\">print(<span class=\"string\">\"straitified cross validation scores:&#123;&#125;\"</span>.format(scores))</span><br><span class=\"line\">print(<span class=\"string\">\"Mean score of straitified cross validation:&#123;:.2f&#125;\"</span>.format(scores.mean()))</span><br><span class=\"line\"><span class=\"comment\"># Leave-one-out Cross-validation 留一法</span></span><br><span class=\"line\"><span class=\"comment\"># 是一种特殊的交叉验证方式。</span></span><br><span class=\"line\"><span class=\"comment\"># 顾名思义，如果样本容量为n，则k=n，进行n折交叉验证，</span></span><br><span class=\"line\"><span class=\"comment\"># 每次留下一个样本进行验证。主要针对小样本数据。</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> LeaveOneOut , cross_val_score</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">import</span>  warnings</span><br><span class=\"line\"> </span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">'ignore'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\">logreg = LogisticRegression()</span><br><span class=\"line\">loout = LeaveOneOut()</span><br><span class=\"line\">scores = cross_val_score(logreg,iris.data,iris.target,cv=loout)</span><br><span class=\"line\">print(<span class=\"string\">\"leave-one-out cross validation scores:&#123;&#125;\"</span>.format(scores))</span><br><span class=\"line\">print(<span class=\"string\">\"Mean score of leave-one-out cross validation:&#123;:.2f&#125;\"</span>.format(scores.mean()))</span><br><span class=\"line\"><span class=\"comment\"># Shuffle-split cross-validation</span></span><br><span class=\"line\"><span class=\"comment\"># 控制更加灵活，可以控制划分迭代次数，</span></span><br><span class=\"line\"><span class=\"comment\"># 每次划分测试集和训练集的比例</span></span><br><span class=\"line\"><span class=\"comment\"># （也就说：可以存在机不再训练集也不再测试集的情况）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.datasets <span class=\"keyword\">import</span> load_iris</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> ShuffleSplit,cross_val_score</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.linear_model <span class=\"keyword\">import</span> LogisticRegression</span><br><span class=\"line\"><span class=\"keyword\">import</span> warnings</span><br><span class=\"line\"> </span><br><span class=\"line\">warnings.filterwarnings(<span class=\"string\">'ignore'</span>)</span><br><span class=\"line\"> </span><br><span class=\"line\">iris = load_iris()</span><br><span class=\"line\"><span class=\"comment\"># 迭代八次</span></span><br><span class=\"line\">shufsp1 = ShuffleSplit(train_size=<span class=\"number\">0.5</span>,test_size=<span class=\"number\">0.4</span>,n_splits=<span class=\"number\">8</span>)</span><br><span class=\"line\">logreg = LogisticRegression()</span><br><span class=\"line\">scores = cross_val_score(logreg,iris.data,iris.target,cv=shufsp1)</span><br><span class=\"line\"> </span><br><span class=\"line\">print(<span class=\"string\">\"shuffle split cross validation scores:\\n&#123;&#125;\"</span>.format(scores))</span><br><span class=\"line\">print(<span class=\"string\">\"Mean score of shuffle split cross validation:&#123;:.2f&#125;\"</span>.format(scores.mean()))</span><br></pre></td></tr></table></figure>","categories":[{"name":"Python","slug":"Python","count":5,"path":"api/categories/Python.json"}],"tags":[{"name":"Python","slug":"Python","count":5,"path":"api/tags/Python.json"},{"name":"sklearn","slug":"sklearn","count":1,"path":"api/tags/sklearn.json"},{"name":"机器学习","slug":"机器学习","count":1,"path":"api/tags/机器学习.json"}]}